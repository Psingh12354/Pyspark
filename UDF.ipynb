{"cells":[{"cell_type":"code","source":["# PySpark UDF is a User Defined Function that is used to create a reusable function in Spark. Once UDF created, that can be re-used on multiple DataFrames and SQL (after registering). The default type of the udf() is StringType.\n# for more knowledge\nhttps://www.geeksforgeeks.org/how-to-write-spark-udf-user-defined-functions-in-python/"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a54f3715-23be-49a3-bc1f-57164bece72d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col,lit,udf\nfrom pyspark.sql.types import IntegerType, DoubleType\nspark = SparkSession.builder.appName(\"UDF\").getOrCreate()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a969933a-e041-40d5-91d9-6eca197b2382"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df=spark.read.options(header='True', inferSchema='True').csv('/FileStore/tables/file_csv.txt')\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7238009d-923a-47f6-8435-973ead282f9d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|        James|     Sales|   NY| 90000| 34|10000|\n|      Michael|     Sales|   NY| 86000| 56|20000|\n|       Robert|     Sales|   CA| 81000| 30|23000|\n|        Maria|   Finance|   CA| 90000| 24|23000|\n|        Raman|   Finance|   CA| 99000| 40|24000|\n|        Scott|   Finance|   NY| 83000| 36|19000|\n|          Jen|   Finance|   NY| 79000| 53|15000|\n|         Jeff| Marketing|   CA| 80000| 25|18000|\n|        Kumar| Marketing|   NY| 91000| 50|21000|\n+-------------+----------+-----+------+---+-----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|        James|     Sales|   NY| 90000| 34|10000|\n|      Michael|     Sales|   NY| 86000| 56|20000|\n|       Robert|     Sales|   CA| 81000| 30|23000|\n|        Maria|   Finance|   CA| 90000| 24|23000|\n|        Raman|   Finance|   CA| 99000| 40|24000|\n|        Scott|   Finance|   NY| 83000| 36|19000|\n|          Jen|   Finance|   NY| 79000| 53|15000|\n|         Jeff| Marketing|   CA| 80000| 25|18000|\n|        Kumar| Marketing|   NY| 91000| 50|21000|\n+-------------+----------+-----+------+---+-----+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["#UDF -> User Defined Functions(Custom Requirements)\n#Simple UDF\n#increase salary for each employee by state\n#if state is NY -> salary by 10% and bonus by 5%\n#if state is CA -> salary by 12% and bonus by 3%\ndef priyanshu(state,salary,bonus):\n demo = 0 #sum\n if state == \"NY\":\n     demo = salary * 0.10\n     demo += bonus * 0.05\n elif state == \"CA\":\n     demo = salary * 0.12\n     demo += bonus * 0.03\n return demo\n \n "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4b039e65-8999-4b60-9746-041b65fd83b4"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["print(priyanshu(\"CA\",1000,100))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"97fb7d89-4f78-4ddb-b0ff-dd658f8bf473"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"123.0\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["123.0\n"]}}],"execution_count":0},{"cell_type":"code","source":["# so basically normal withcol perform operation on whole col while through udf we can apply condition \n# here it is taking two input first one is the format and second one is the return type\nUdf = udf(lambda x,y,z: priyanshu(x,y,z), DoubleType())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1de98ab6-1bc4-43b0-aa34-1d328739a77b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# here we are creating new column increment and passing value from csv\ndf.withColumn(\"increment\",Udf(df.state,df.salary,df.bonus)).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6aa9d185-3488-47a7-904d-9fb2f0322482"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------------+----------+-----+------+---+-----+---------+\n|employee_name|department|state|salary|age|bonus|increment|\n+-------------+----------+-----+------+---+-----+---------+\n|        James|     Sales|   NY| 90000| 34|10000|   9500.0|\n|      Michael|     Sales|   NY| 86000| 56|20000|   9600.0|\n|       Robert|     Sales|   CA| 81000| 30|23000|  10410.0|\n|        Maria|   Finance|   CA| 90000| 24|23000|  11490.0|\n|        Raman|   Finance|   CA| 99000| 40|24000|  12600.0|\n|        Scott|   Finance|   NY| 83000| 36|19000|   9250.0|\n|          Jen|   Finance|   NY| 79000| 53|15000|   8650.0|\n|         Jeff| Marketing|   CA| 80000| 25|18000|  10140.0|\n|        Kumar| Marketing|   NY| 91000| 50|21000|  10150.0|\n+-------------+----------+-----+------+---+-----+---------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------------+----------+-----+------+---+-----+---------+\n|employee_name|department|state|salary|age|bonus|increment|\n+-------------+----------+-----+------+---+-----+---------+\n|        James|     Sales|   NY| 90000| 34|10000|   9500.0|\n|      Michael|     Sales|   NY| 86000| 56|20000|   9600.0|\n|       Robert|     Sales|   CA| 81000| 30|23000|  10410.0|\n|        Maria|   Finance|   CA| 90000| 24|23000|  11490.0|\n|        Raman|   Finance|   CA| 99000| 40|24000|  12600.0|\n|        Scott|   Finance|   NY| 83000| 36|19000|   9250.0|\n|          Jen|   Finance|   NY| 79000| 53|15000|   8650.0|\n|         Jeff| Marketing|   CA| 80000| 25|18000|  10140.0|\n|        Kumar| Marketing|   NY| 91000| 50|21000|  10150.0|\n+-------------+----------+-----+------+---+-----+---------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col,lit,udf\nfrom pyspark.sql.types import IntegerType, DoubleType,ArrayType\nspark = SparkSession.builder.appName(\"UDF\").getOrCreate()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c02f5687-8b91-48aa-a8e8-d0dfcaac9a3d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#UDF -> User Defined Functions(Custom Requirements)\n#Simple UDF\n#increase salary for each employee by state\n#if state is NY -> salary by 10% and bonus by 5%\n#if state is CA -> salary by 12% and bonus by 3%\ndef priyanshu(state,salary,bonus):\n demo1 = 0 #sum\n demo2 = 0\n if state == \"NY\":\n     demo1 = salary * 0.10\n     demo2 = bonus * 0.05\n elif state == \"CA\":\n     demo1 = salary * 0.12\n     demo2 = bonus * 0.03\n return demo1,demo2\n \n "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"60b83bab-2fb3-401f-8a79-2d2c7d143c56"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["Udf = udf(lambda x,y,z: priyanshu(x,y,z), ArrayType(DoubleType()))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0128f49c-e43b-4ec9-a95f-95aa055b8e60"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# here we are creating new column increment and passing value from csv\ndemo = df.withColumn(\"increment\",Udf(df.state,df.salary,df.bonus))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"22b724a1-a346-4f91-b8c0-b36ca50fbe99"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# cache\ndemo1 = demo\nprint(type(demo1))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"668e85e0-ed3e-4919-ab88-b66be68165a9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<class 'pyspark.sql.dataframe.DataFrame'>\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["<class 'pyspark.sql.dataframe.DataFrame'>\n"]}}],"execution_count":0},{"cell_type":"code","source":["# cache\ndemo2 = demo1\nprint(type(demo2))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"56fca56d-ce34-4e26-b8c7-7c485aef74c7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<class 'pyspark.sql.dataframe.DataFrame'>\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["<class 'pyspark.sql.dataframe.DataFrame'>\n"]}}],"execution_count":0},{"cell_type":"code","source":["# cache\ndemo3 = demo2\nprint(type(demo3))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e391d0da-8946-4958-8fd6-eba58f922dd1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<class 'pyspark.sql.dataframe.DataFrame'>\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["<class 'pyspark.sql.dataframe.DataFrame'>\n"]}}],"execution_count":0},{"cell_type":"code","source":["demo1.cache()\ndemo2.cache()\ndemo3.cache()\ndemo3.persist()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9648fa8f-2899-42a5-96d4-c80748e4ec7d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[30]: DataFrame[employee_name: string, department: string, state: string, salary: int, age: int, bonus: int, increment: array<double>]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[30]: DataFrame[employee_name: string, department: string, state: string, salary: int, age: int, bonus: int, increment: array<double>]"]}}],"execution_count":0},{"cell_type":"code","source":["# By default transformation is lazy in nature means it's always start from very begining even if you run them  multiple time it wont save the data like data->demo1->demo2->demo3 so even if you already run demo1 than also demo2 will run demo1 and demo3 will run demo2 and demo1 and fetch the data. \n# So to overcome we save the data in either Ram(Cache) or Internal hard drive like (Persist)\n# Using cache() and persist() methods, Spark provides an optimization mechanism to store the intermediate computation of an RDD, DataFrame, and Dataset so they can be reused in subsequent actions(reusing the RDD, Dataframe, and Dataset computation result’s).\n\n# Both caching and persisting are used to save the Spark RDD, Dataframe, and Dataset’s. But, the difference is, RDD cache() method default saves it to memory (MEMORY_ONLY) whereas persist() method is used to store it to the user-defined storage level."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"da243651-141d-4258-a534-6f9f9713d803"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"UDF","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":4249106993945900}},"nbformat":4,"nbformat_minor":0}
