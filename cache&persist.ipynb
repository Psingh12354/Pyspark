{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84743fbd-edee-40f1-8b74-0ec5700cd50c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üß† PySpark `cache()` and `persist()` ‚Äì Notes\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Purpose of `.cache()` and `.persist()`\n",
    "\n",
    "### ‚öôÔ∏è 1. Avoid Recomputing Expensive Operations\n",
    "\n",
    "- PySpark transformations are **lazy** ‚Äì nothing is executed until an **action** is called.\n",
    "- If you reuse the same DataFrame or RDD multiple times, Spark **recomputes the full DAG** every time.\n",
    "- This can be slow and inefficient for large or complex transformations.\n",
    "\n",
    "#### üîÅ Example (Without Caching):\n",
    "\n",
    "```python\n",
    "df_filtered = df.filter(\"age > 30\")\n",
    "\n",
    "# First action\n",
    "df_filtered.groupBy(\"city\").count().show()\n",
    "\n",
    "# Second action ‚Äì recomputes the filter again!\n",
    "df_filtered.agg({\"salary\": \"avg\"}).show()\n",
    "```\n",
    "---\n",
    "\n",
    "## üîÅ `cache()` in PySpark\n",
    "\n",
    "- Shortcut for: `.persist(StorageLevel.MEMORY_AND_DISK)`\n",
    "- Stores DataFrame or RDD in **memory**, and if memory is full, **spills to disk**.\n",
    "- Best for datasets used multiple times in actions or transformations.\n",
    "\n",
    "```python\n",
    "df = spark.read.csv(\"data.csv\", header=True, inferSchema=True)\n",
    "df.cache()  # Default = MEMORY_AND_DISK\n",
    "df.count()  # Triggers the cache\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üßä `persist()` in PySpark\n",
    "\n",
    "- Gives you **full control** over the storage level.\n",
    "- You can persist data in memory, on disk, or in serialized form.\n",
    "\n",
    "```python\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "df.persist(StorageLevel.MEMORY_ONLY)\n",
    "df.count()  # Triggers caching\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Storage Levels for `.persist()`\n",
    "\n",
    "| Storage Level              | Description                                      |\n",
    "|---------------------------|--------------------------------------------------|\n",
    "| `MEMORY_ONLY`             | Fastest; data must fit in memory                 |\n",
    "| `MEMORY_AND_DISK`         | Tries memory; spills to disk if needed           |\n",
    "| `DISK_ONLY`               | Stores only on disk (slowest)                    |\n",
    "| `MEMORY_ONLY_SER`         | Stores serialized objects in memory              |\n",
    "| `MEMORY_AND_DISK_SER`     | Serialized in memory; spills to disk if needed   |\n",
    "| `OFF_HEAP`                | Uses off-heap memory (advanced use)              |\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ `unpersist()` ‚Äì Clear Cache\n",
    "\n",
    "- Removes data from memory/disk:\n",
    "\n",
    "```python\n",
    "df.unpersist()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Example: Using `persist()` and `unpersist()`\n",
    "\n",
    "```python\n",
    "df = spark.read.csv(\"employees.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Filter and persist intermediate result\n",
    "filtered_df = df.filter(\"salary > 50000\").persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "# Reuse multiple times\n",
    "filtered_df.groupBy(\"department\").count().show()\n",
    "filtered_df.agg({\"salary\": \"avg\"}).show()\n",
    "\n",
    "# Free up memory\n",
    "filtered_df.unpersist()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ When Should You Use It?\n",
    "\n",
    "- Use `.cache()`:\n",
    "  - When default memory+disk is sufficient\n",
    "- Use `.persist(level)`:\n",
    "  - When you need custom control (e.g., memory only, disk only)\n",
    "- Always trigger with an **action** like `.count()`, `.show()`, `.collect()`, etc.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7466b864-a96a-4706-8cc3-a36fc07ee908",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "com.databricks.backend.daemon.driver.DriverClientDestroyedException: abort: DriverClient destroyed\n",
       "\tat com.databricks.backend.daemon.driver.DriverClient.$anonfun$poll$4(DriverClient.scala:797)\n",
       "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
       "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:54)\n",
       "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:77)\n",
       "\tat com.databricks.threading.ContextBoundRunnable.$anonfun$run$2(ContextBoundRunnable.scala:16)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:291)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:287)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.threading.ContextBoundRunnable.withAttributionContext(ContextBoundRunnable.scala:7)\n",
       "\tat com.databricks.threading.ContextBoundRunnable.$anonfun$run$1(ContextBoundRunnable.scala:16)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n",
       "\tat com.databricks.threading.ContextBoundRunnable.run(ContextBoundRunnable.scala:15)\n",
       "\tat com.databricks.threading.DatabricksExecutionContext$InstrumentedRunnable.run(DatabricksExecutionContext.scala:36)\n",
       "\tat com.databricks.threading.NamedExecutor$$anon$3.run(NamedExecutor.scala:750)\n",
       "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
       "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:842)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "com.databricks.backend.daemon.driver.DriverClientDestroyedException: abort: DriverClient destroyed\n\tat com.databricks.backend.daemon.driver.DriverClient.$anonfun$poll$4(DriverClient.scala:797)\n\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:54)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:77)\n\tat com.databricks.threading.ContextBoundRunnable.$anonfun$run$2(ContextBoundRunnable.scala:16)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:291)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:287)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.threading.ContextBoundRunnable.withAttributionContext(ContextBoundRunnable.scala:7)\n\tat com.databricks.threading.ContextBoundRunnable.$anonfun$run$1(ContextBoundRunnable.scala:16)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n\tat com.databricks.threading.ContextBoundRunnable.run(ContextBoundRunnable.scala:15)\n\tat com.databricks.threading.DatabricksExecutionContext$InstrumentedRunnable.run(DatabricksExecutionContext.scala:36)\n\tat com.databricks.threading.NamedExecutor$$anon$3.run(NamedExecutor.scala:750)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
       "errorSummary": "Internal error. Attach your notebook to a different compute or restart the current compute.",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# 1. Start Spark session\n",
    "spark = SparkSession.builder.appName(\"CachePersistExample\").getOrCreate()\n",
    "\n",
    "# 2. Create sample employee data\n",
    "data = [\n",
    "    (\"e01\", \"Alice\", \"HR\", 55000),\n",
    "    (\"e02\", \"Bob\", \"IT\", 48000),\n",
    "    (\"e03\", \"Carol\", \"Finance\", 62000),\n",
    "    (\"e04\", \"David\", \"IT\", 72000),\n",
    "    (\"e05\", \"Eva\", \"HR\", 51000),\n",
    "    (\"e06\", \"Frank\", \"Finance\", 45000),\n",
    "    (\"e07\", \"Grace\", \"IT\", 53000)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"department\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# 3. Create original DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# 4. Filter high-salary employees\n",
    "high_salary_df = df.filter(\"salary > 50000\")\n",
    "\n",
    "# Rename columns in df to avoid ambiguity during join\n",
    "df = df.withColumnRenamed('name', 'namedf') \\\n",
    "       .withColumnRenamed('department', 'departmentdf') \\\n",
    "       .withColumnRenamed('salary', 'salarydf')\n",
    "\n",
    "# 5. Cache the filtered DataFrame\n",
    "high_salary_df.cache()\n",
    "\n",
    "# 6. Trigger cache with an action\n",
    "print(\"üîÑ Counting high-salary employees:\")\n",
    "print(\"Total:\", high_salary_df.count())\n",
    "\n",
    "# 7. Reuse cached DataFrame\n",
    "print(\"\\nüìä Department-wise count of high earners:\")\n",
    "high_salary_df.groupBy(\"department\").count().show()\n",
    "\n",
    "print(\"üí∞ Average salary of high earners:\")\n",
    "high_salary_df.agg({\"salary\": \"avg\"}).show()\n",
    "\n",
    "# 8. Unpersist cached DataFrame\n",
    "high_salary_df.unpersist()\n",
    "\n",
    "# 9. Join original (renamed) df with high_salary_df on \"id\"\n",
    "joined_df = df.join(high_salary_df, on=\"id\", how=\"inner\")\n",
    "\n",
    "# 10. Persist the joined DataFrame with available storage level\n",
    "joined_df.persist(StorageLevel.MEMORY_AND_DISK_DESER)\n",
    "\n",
    "# 11. Trigger persist with an action and show joined data\n",
    "print(\"\\nüîÅ Joined high salary employees (with disambiguated columns):\")\n",
    "joined_df.select(\n",
    "    \"id\",\n",
    "    \"namedf\",         # from df\n",
    "    \"departmentdf\",   # from df\n",
    "    \"salarydf\",       # from df\n",
    "    \"name\",           # from high_salary_df\n",
    "    \"department\",     # from high_salary_df\n",
    "    \"salary\"          # from high_salary_df\n",
    ").show()\n",
    "\n",
    "# 12. Unpersist the joined DataFrame\n",
    "joined_df.unpersist()\n",
    "\n",
    "# 13. Stop Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "165d5532-25b4-4fbc-b064-af9eaf5eef30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DISK_ONLY', 'DISK_ONLY_2', 'DISK_ONLY_3', 'MEMORY_AND_DISK', 'MEMORY_AND_DISK_2', 'MEMORY_AND_DISK_DESER', 'MEMORY_ONLY', 'MEMORY_ONLY_2', 'OFF_HEAP', '__annotations__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__']\n"
     ]
    }
   ],
   "source": [
    "print(dir(StorageLevel))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2025-07-11 15:21:24",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}