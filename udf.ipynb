{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05010f77-aa6e-4294-a013-a4d7a11f9fe4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üîß PySpark UDF (User Defined Function) - Notes\n",
    "\n",
    "## üß† What is a UDF?\n",
    "\n",
    "A **UDF (User Defined Function)** in PySpark allows you to write custom Python functions that can be applied to columns in a DataFrame.\n",
    "\n",
    "- Useful when built-in PySpark functions are not sufficient.\n",
    "- Can be used in transformations like `withColumn`, `select`, and `filter`.\n",
    "- Operates row-by-row like Python‚Äôs `map` function.\n",
    "\n",
    "> ‚ö†Ô∏è **Note:** UDFs are generally slower than built-in functions because they bypass Spark‚Äôs Catalyst optimizer and require serialization.\n",
    "\n",
    "---\n",
    "\n",
    "## üß± Steps to Use a UDF in PySpark\n",
    "\n",
    "1. **Import necessary libraries** (like `udf` and data types).\n",
    "2. **Define your custom Python function.**\n",
    "3. **Register the function as a UDF**, specifying the return type.\n",
    "4. **Apply the UDF** on DataFrame columns using transformations like `withColumn`.\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Sample Use Case\n",
    "\n",
    "Imagine a situation where you want to assign age categories such as \"Young\", \"Adult\", and \"Senior\" based on age. This logic is not available directly in Spark functions, so you would use a UDF.\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Best Practices\n",
    "\n",
    "- **Prefer Spark SQL built-in functions** whenever possible ‚Äî they are faster and optimized.\n",
    "- Use **UDFs only when**:\n",
    "  - You need custom logic not supported by built-in functions.\n",
    "  - Business rules require non-standard computations.\n",
    "- Consider **Pandas UDFs (vectorized UDFs)** for better performance if you're using Spark 2.3 or later.\n",
    "- UDFs cannot be automatically optimized or pushed down into query plans.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Bonus: SQL UDFs\n",
    "\n",
    "You can also **register UDFs** to use inside **Spark SQL queries**, allowing SQL users to apply the same Python logic in their workflows.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ UDFs extend the power of Spark by allowing custom logic, but they should be used wisely for performance-sensitive applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbb47a77-6db2-42e0-be4c-969435ec12d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df0cea88-99bb-47f5-86ba-02b03ea9714a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"UDF Example\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "331b8efe-9ae1-4ef1-b3ad-e4fbe7de2a23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n| name|age|\n+-----+---+\n|Alice| 21|\n|  Bob| 25|\n|Cathy| 30|\n+-----+---+\n\n"
     ]
    }
   ],
   "source": [
    "data = [(\"Alice\", 21), (\"Bob\", 25), (\"Cathy\", 30)]\n",
    "columns = [\"name\", \"age\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1295443-5a61-460c-a68b-5d333af21c54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def age_category(age):\n",
    "    if age < 25:\n",
    "        return \"Young\"\n",
    "    elif age < 30:\n",
    "        return \"Adult\"\n",
    "    else:\n",
    "        return \"Senior\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a561bcd6-577d-4535-8182-972a5fc16cc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+--------+\n| name|age|category|\n+-----+---+--------+\n|Alice| 21|   Young|\n|  Bob| 25|   Adult|\n|Cathy| 30|  Senior|\n+-----+---+--------+\n\n"
     ]
    }
   ],
   "source": [
    "age_category_udf = udf(age_category, StringType())\n",
    "df_with_category = df.withColumn(\"category\", age_category_udf(df[\"age\"]))\n",
    "df_with_category.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d0ec5db-97e0-4c33-990b-082ad88a451d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Spark Sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b1e1f47-aecf-4e43-9805-34b2f15b3117",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+--------+\n| name|age|category|\n+-----+---+--------+\n|Alice| 21|   Young|\n|  Bob| 25|   Adult|\n|Cathy| 30|  Senior|\n+-----+---+--------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.udf.register(\"age_category_sql\", age_category, StringType())\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "spark.sql(\"SELECT name, age, age_category_sql(age) AS category FROM people\").show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2025-07-10 16:16:42",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}