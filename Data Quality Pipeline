---

# ğŸ§± Data Quality Pipeline in Databricks (PySpark + Delta Lake)

---

## ğŸ“˜ Table of Contents

1. [Overview](#overview)
2. [Architecture](#architecture)
3. [Data Lake Zones](#data-lake-zones)
4. [Step-by-Step Implementation](#step-by-step-implementation)

   * [Step 1: Load Raw Data (Bronze)](#step-1-load-raw-data-bronze)
   * [Step 2: Define Data Quality Rules](#step-2-define-data-quality-rules)
   * [Step 3: Validate Data](#step-3-validate-data)
   * [Step 4: Split into Silver and Quarantine](#step-4-split-into-silver-and-quarantine)
   * [Step 5: Add Metadata for Rejection Reason](#step-5-add-metadata-for-rejection-reason)
   * [Step 6: Apply Delta Constraints](#step-6-apply-delta-constraints)
   * [Step 7: Track Data Quality Metrics](#step-7-track-data-quality-metrics)
   * [Step 8: Repair and Reprocess Quarantined Data](#step-8-repair-and-reprocess-quarantined-data)
5. [Folder Structure](#folder-structure)
6. [Summary Table](#summary-table)
7. [Enhancements](#enhancements)

---

## ğŸ§  Overview

Data Quality ensures that your data is:

* âœ… **Accurate** â€“ Represents real-world facts correctly
* âœ… **Complete** â€“ No missing key values
* âœ… **Consistent** â€“ Same format, same meaning
* âœ… **Valid** â€“ Meets business rules
* âœ… **Unique** â€“ No duplicates
* âœ… **Timely** â€“ Up to date

This notebook demonstrates how to implement a **Data Quality Framework** using **PySpark** and **Delta Lake** on **Databricks**, including handling **quarantined (invalid)** records.

---

## ğŸ—ï¸ Architecture

A modern Databricks data quality pipeline follows a **multi-layer architecture**:

```
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚       Source Systems       â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚  ğŸŸ¤ Bronze Layer   â”‚ â†’ Raw Ingested Data
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚  âšª Silver Layer      â”‚ â†’ Valid & Clean Data
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â–¼                             â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ ğŸŸ¥ Quarantine     â”‚         â”‚ ğŸŸ¡ Gold Layer       â”‚
   â”‚ Invalid Data      â”‚         â”‚ Aggregated Reports  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§© Data Lake Zones

| Zone              | Description                      | Example Table             |
| ----------------- | -------------------------------- | ------------------------- |
| ğŸŸ¤ **Bronze**     | Raw ingested data, no validation | `bronze.transactions`     |
| âšª **Silver**      | Cleaned, validated data          | `silver.transactions`     |
| ğŸŸ¥ **Quarantine** | Invalid/rejected records         | `quarantine.transactions` |
| ğŸŸ¡ **Gold**       | Aggregated, business-ready data  | `gold.sales_summary`      |

---

## ğŸ§± Step-by-Step Implementation

---

### ğŸ¥‰ Step 1: Load Raw Data (Bronze)

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("DataQualityPipeline").getOrCreate()

data = [
    (101, 250, "2025-10-30", "a@gmail.com"),
    (102, None, "2025-10-30", "b@gmail.com"),
    (103, 2000, "2025-13-30", "c@@gmail.com"),
    (101, 250, "2025-10-30", "a@gmail.com")
]
columns = ["customer_id", "amount", "transaction_date", "email"]

df_bronze = spark.createDataFrame(data, columns)

df_bronze.write.format("delta").mode("overwrite").saveAsTable("bronze.transactions")
```

---

### âš™ï¸ Step 2: Define Data Quality Rules

| Rule ID | Column           | Description                       | Rule                                                  |
| ------- | ---------------- | --------------------------------- | ----------------------------------------------------- |
| R1      | amount           | Must not be null or negative      | `amount IS NOT NULL AND amount > 0`                   |
| R2      | transaction_date | Must be a valid date (YYYY-MM-DD) | `to_date(transaction_date, 'yyyy-MM-dd') IS NOT NULL` |
| R3      | email            | Must be valid email format        | `email LIKE '%@%.%'`                                  |
| R4      | duplicates       | Remove duplicate transactions     | `dropDuplicates(['customer_id', 'transaction_date'])` |

---

### ğŸ§ª Step 3: Validate Data

```python
from pyspark.sql.functions import col, to_date

df_valid = (
    df_bronze
    .dropDuplicates(["customer_id", "transaction_date"])
    .filter(col("amount").isNotNull() & (col("amount") > 0))
    .filter(to_date(col("transaction_date"), "yyyy-MM-dd").isNotNull())
    .filter(col("email").rlike("^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$"))
)

df_invalid = df_bronze.subtract(df_valid)
```

---

### âšª Step 4: Split into Silver and Quarantine

```python
# âœ… Valid records â†’ Silver
df_valid.write.format("delta").mode("overwrite").saveAsTable("silver.transactions")

# âŒ Invalid records â†’ Quarantine
df_invalid.write.format("delta").mode("overwrite").saveAsTable("quarantine.transactions")
```

---

### ğŸ§¾ Step 5: Add Metadata for Rejection Reason

```python
from pyspark.sql.functions import when, lit

df_with_status = (
    df_bronze
    .withColumn(
        "validation_status",
        when(col("amount").isNull() | (col("amount") <= 0), lit("Invalid or Missing Amount"))
        .when(to_date(col("transaction_date"), "yyyy-MM-dd").isNull(), lit("Invalid Date"))
        .when(~col("email").rlike("^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$"), lit("Invalid Email"))
        .otherwise(lit("Valid"))
    )
)

df_valid = df_with_status.filter(col("validation_status") == "Valid").drop("validation_status")
df_invalid = df_with_status.filter(col("validation_status") != "Valid")

df_valid.write.format("delta").mode("overwrite").saveAsTable("silver.transactions")
df_invalid.write.format("delta").mode("overwrite").saveAsTable("quarantine.transactions")
```

ğŸŸ¥ **Sample Quarantine Table Output:**

| customer_id | amount | transaction_date | email                             | validation_status         |
| ----------- | ------ | ---------------- | --------------------------------- | ------------------------- |
| 102         | NULL   | 2025-10-30       | [b@gmail.com](mailto:b@gmail.com) | Invalid or Missing Amount |
| 103         | 2000   | 2025-13-30       | c@@gmail.com                      | Invalid Date              |
| 103         | 2000   | 2025-13-30       | c@@gmail.com                      | Invalid Email             |

---

### ğŸ”’ Step 6: Apply Delta Constraints (for Silver)

```python
spark.sql("""
ALTER TABLE silver.transactions
SET TBLPROPERTIES (
  'delta.constraints.amount_check' = 'amount > 0',
  'delta.constraints.valid_email' = "email LIKE '%@%.%'"
)
""")
```

If any record violates these constraints during insert, the transaction **fails** â€” ensuring **data integrity**.

---

### ğŸ“Š Step 7: Track Data Quality Metrics

```python
total_records = df_bronze.count()
valid_records = df_valid.count()
invalid_records = df_invalid.count()

dq_score = (valid_records / total_records) * 100

print(f"âœ… Data Quality Score: {dq_score:.2f}%")
print(f"Valid Records: {valid_records}, Invalid Records: {invalid_records}")
```

ğŸ§® **Example Output:**

```
âœ… Data Quality Score: 50.00%
Valid Records: 2, Invalid Records: 2
```

---

### ğŸ§° Step 8: Repair and Reprocess Quarantined Data

Sometimes quarantined data can be **fixed** and **re-ingested**.

Example: Missing amount or typo in email.

```python
from pyspark.sql.functions import regexp_replace

df_quarantine = spark.table("quarantine.transactions")

# Example fixes
df_repaired = (
    df_quarantine
    .withColumn("amount", when(col("amount").isNull(), lit(100)).otherwise(col("amount")))
    .withColumn("email", regexp_replace(col("email"), "@@", "@"))
    .filter(col("validation_status").isin("Invalid or Missing Amount", "Invalid Email"))
    .drop("validation_status")
)

# Move fixed data to Silver
df_repaired.write.format("delta").mode("append").saveAsTable("silver.transactions")
```

---

## ğŸ“ Folder Structure

A standard folder layout in your **Data Lake (ADLS/DBFS)**:

```
/mnt/datalake/
 â”œâ”€â”€ bronze/
 â”‚    â””â”€â”€ transactions/
 â”œâ”€â”€ silver/
 â”‚    â””â”€â”€ transactions/
 â”œâ”€â”€ quarantine/
 â”‚    â””â”€â”€ transactions/
 â””â”€â”€ gold/
      â””â”€â”€ sales_summary/
```

---

## ğŸ“˜ Summary Table

| Step | Action              | Output Table             | Tool/Method       |
| ---- | ------------------- | ------------------------ | ----------------- |
| 1    | Load Raw Data       | `bronze.transactions`    | PySpark           |
| 2    | Define Rules        | Validation Logic         | Python/PySpark    |
| 3    | Validate Data       | `df_valid`, `df_invalid` | Filters & Regex   |
| 4    | Write to Zones      | Silver + Quarantine      | Delta Write       |
| 5    | Track Reasons       | Add `validation_status`  | `withColumn()`    |
| 6    | Enforce Constraints | Table Properties         | Delta Constraints |
| 7    | Compute Metrics     | DQ Score                 | `count()`         |
| 8    | Reprocess Invalids  | Move Fixed Data          | Delta Append      |

---

## ğŸš€ Enhancements (Production-Ready Ideas)

| Enhancement                       | Description                                              |
| --------------------------------- | -------------------------------------------------------- |
| ğŸ§± **Delta Live Tables (DLT)**    | Use `@dlt.expect_or_drop()` to automate data validation  |
| ğŸ“Š **Data Quality Dashboard**     | Create Databricks SQL dashboard for DQ score trends      |
| ğŸ§  **Great Expectations / Deequ** | Integrate for advanced rule checks & profiling           |
| âš™ï¸ **Job Orchestration**          | Use Databricks Workflows or Airflow to schedule pipeline |
| ğŸ”” **Alerting**                   | Trigger alerts when invalid % > threshold                |

---

## âœ… Final Takeaway

This pattern (Silver + Quarantine) is the **industry-standard data quality architecture** in Databricks pipelines:

* ğŸŸ¤ **Bronze:** Raw, unvalidated data
* âšª **Silver:** Clean and validated data
* ğŸŸ¥ **Quarantine:** Invalid data (with rejection reasons)
* ğŸŸ¡ **Gold:** Business-ready analytics

You now have a **fully functional and production-ready Data Quality framework** â€” end-to-end in PySpark and Delta Lake ğŸš€

---

Would you like me to extend this note with a **Delta Live Tables (DLT)** version â€” where Databricks automatically tracks validation metrics and quarantine counts in the DLT UI? Itâ€™s often asked in senior-level Databricks interviews.
