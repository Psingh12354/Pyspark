{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "163e1c28-6f2f-427b-bf4c-b9dd8946e5e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=562437900755898#setting/sparkui/0707-102323-d9uua2rq/driver-4562856129145811045\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=562437900755898#setting/sparkui/0707-102323-d9uua2rq/driver-4562856129145811045\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, explode, size\n",
    "from pyspark.sql.types import StructType, StringType, IntegerType, ArrayType\n",
    "SparkSession.builder.appName(\"Spark DF\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83deb494-8dc7-49e7-a8c8-b3edf23e7069",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sampleDf = [\n",
    "  { \"id\": 1, \"name\": \"Alice\", \"age\": 30 },\n",
    "  { \"id\": 2, \"name\": \"Bob\", \"age\": 25 },\n",
    "  { \"id\": 3, \"name\": \"Charlie\", \"age\": 35 }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79e8ab1e-ae93-4ce6-af84-8d4fba1f79c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "> For accessing below like data use dot notation col(\"address.city\") or select(\"address.city\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "faf9cc77-e007-49bc-85c6-ad70b28ef60c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sampleDf = [\n",
    "  {\n",
    "    \"id\": 1,\n",
    "    \"name\": \"Alice\",\n",
    "    \"address\": {\n",
    "      \"city\": \"New York\",\n",
    "      \"zip\": \"10001\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"id\": 2,\n",
    "    \"name\": \"Bob\",\n",
    "    \"address\": {\n",
    "      \"city\": \"Los Angeles\",\n",
    "      \"zip\": \"90001\"\n",
    "    }\n",
    "  }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff6ed5ab-04f7-453a-aa44-48212e49fd85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(sampleDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f9c385a-7378-4785-98a1-1565f7071c2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- age: long (nullable = true)\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28f181c8-e325-423b-b7b7-b04d12552e5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+-----+\n|             address| id| name|\n+--------------------+---+-----+\n|{zip -> 10001, ci...|  1|Alice|\n|{zip -> 90001, ci...|  2|  Bob|\n+--------------------+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a6b560a-2d76-4ed7-9c04-947bc9842e58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n|  zip|\n+-----+\n|10001|\n|90001|\n+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(col('address.zip')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84de4186-8dd4-43f7-b727-dd4a47b7c45a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Json with Arrays\n",
    "\n",
    "- explode(\"skills\")\n",
    "\n",
    "- Array functions like size(), array_contains()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bf27696-7cb2-4b9a-969e-b4f22468d02b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sampleDf = [\n",
    "  {\n",
    "    \"id\": 1,\n",
    "    \"name\": \"Alice\",\n",
    "    \"skills\": [\"Python\", \"Spark\", \"SQL\"]\n",
    "  },\n",
    "  {\n",
    "    \"id\": 2,\n",
    "    \"name\": \"Bob\",\n",
    "    \"skills\": [\"Java\", \"Scala\"]\n",
    "  }\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5a18df3-ca7a-46e9-8408-ba47f8215e38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(sampleDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d646180-e5d7-4877-bd9b-7e61216f2b1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n| name|skills|\n+-----+------+\n|Alice|Python|\n|Alice| Spark|\n|Alice|   SQL|\n|  Bob|  Java|\n|  Bob| Scala|\n+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select('name',explode('skills').alias('skills')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a4be713-d489-4528-ad39-b90c2728f31d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------------------+-----------+\n| id| name|              skills|skill_count|\n+---+-----+--------------------+-----------+\n|  1|Alice|[Python, Spark, SQL]|          3|\n|  2|  Bob|       [Java, Scala]|          2|\n+---+-----+--------------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"skill_count\", size(\"skills\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91c70d7f-5d76-4585-acd0-0d72e7943d65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    " Parsing JSON Strings with from_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33d7c49f-5807-4f4c-ab91-4942bd2dd102",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sampleDf = [\n",
    "  {\n",
    "    \"id\": 1,\n",
    "    \"json_data\": \"{\\\"product\\\":\\\"Laptop\\\",\\\"price\\\":900}\"\n",
    "  }\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(sampleDf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a3d7f02-42dd-4dc2-9946-9ebbcc1a7d2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n| id|           json_data|\n+---+--------------------+\n|  1|{\"product\":\"Lapto...|\n+---+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6025f0dd-c600-4c83-87e9-0367498a9c41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----+\n| id|product|price|\n+---+-------+-----+\n|  1| Laptop|  900|\n+---+-------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "schema = StructType() \\\n",
    "    .add(\"product\", StringType()) \\\n",
    "    .add(\"price\", IntegerType())\n",
    "df_parsed = df.withColumn(\"data\", from_json(\"json_data\", schema))\n",
    "df_parsed.select(\"id\", \"data.product\", \"data.price\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "448da530-3b17-4556-b727-ecb07216a967",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    " Handling Nulls & Missing Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5923083-d507-4495-b9e2-207c3702659a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sampleDf = [\n",
    "  { \"id\": 1, \"name\": \"Alice\", \"age\": 30 },\n",
    "  { \"id\": 2, \"name\": None },\n",
    "  { \"id\": 3 }\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(sampleDf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4028c567-09c3-42c5-a813-69d665f0bdba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+-----+\n| age| id| name|\n+----+---+-----+\n|  30|  1|Alice|\n|null|  2| null|\n|null|  3| null|\n+----+---+-----+\n\n+---+---+-------+\n|age| id|   name|\n+---+---+-------+\n| 30|  1|  Alice|\n|  0|  2|Unknown|\n|  0|  3|Unknown|\n+---+---+-------+\n\n+---+---+-----+\n|age| id| name|\n+---+---+-----+\n| 30|  1|Alice|\n+---+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()\n",
    "\n",
    "# Fill nulls\n",
    "df.na.fill({\"name\": \"Unknown\", \"age\": 0}).show()\n",
    "\n",
    "# Drop rows with any nulls\n",
    "df.na.drop().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a883aea7-0c04-4203-8e5b-775b2e094911",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    " Reading Multiline JSON\n",
    " \n",
    " Below you will see 2 approach using **createdataframe** and **read from dbfs**\n",
    "\n",
    " Problem\n",
    " - Without any options, Spark assumes each line is an individual JSON record. So it will fail or parse incorrectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "320eb75f-487f-4c89-b39b-24a609cfc945",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sampleDf = {\n",
    "  \"id\": 1,\n",
    "  \"name\": \"Alice\",\n",
    "  \"details\": {\n",
    "    \"hobbies\": [\"reading\", \"cycling\"],\n",
    "    \"education\": {\n",
    "      \"degree\": \"Masters\",\n",
    "      \"field\": \"Computer Science\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "# pass in a list or wrap the dict\n",
    "df = spark.createDataFrame([sampleDf])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e77a2e2-abfa-49eb-9468-0a5a82b2b9ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+-----+\n|             details| id| name|\n+--------------------+---+-----+\n|{hobbies -> [read...|  1|Alice|\n+--------------------+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09f0425a-2925-42ad-b1ba-7ed29b8d76ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n| name|  hobby|\n+-----+-------+\n|Alice|reading|\n|Alice|cycling|\n+-----+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(\"name\", explode(\"details.hobbies\").alias(\"hobby\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1cfdad8a-dccd-491b-986f-ebacbaa1f110",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "to read from json file use multiline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8491980-5c58-4010-9aa3-6a654b87450d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------+\n| name|           field|\n+-----+----------------+\n|Alice|Computer Science|\n+-----+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"multiline\", \"true\").json(\"dbfs:/FileStore/tables/multiline.json\")\n",
    "df.select(\"name\", \"details.education.field\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5420b166-b0c9-4ebd-b876-8a11b95f9474",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Handling Malformed JSON\n",
    "\n",
    "Problem\n",
    "- Youâ€™re reading a file where some records are corrupted or incomplete. By default, Spark tries to read every line as valid JSON.\n",
    "\n",
    "\n",
    "| Mode            | Behavior                                                  |\n",
    "| --------------- | --------------------------------------------------------- |\n",
    "| `PERMISSIVE`    | Keeps corrupt records in `_corrupt_record` column         |\n",
    "| `DROPMALFORMED` | Skips bad rows altogether                                 |\n",
    "| `FAILFAST`      | Stops execution immediately if any malformed row is found |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abdb30a0-6dec-41a9-a6ea-56fcd4784d12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-----+\n|     _corrupt_record|  id| name|\n+--------------------+----+-----+\n|                null|   1|Alice|\n|                null|   2|  Bob|\n|{ \"id\": 3, \"name\"...|null| null|\n+--------------------+----+-----+\n\n+---+-----+\n| id| name|\n+---+-----+\n|  1|Alice|\n|  2|  Bob|\n+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"mode\", \"PERMISSIVE\").json(\"dbfs:/FileStore/tables/malformed.json\")\n",
    "df.show()\n",
    "\n",
    "# Try DROP mode to skip bad records\n",
    "df_drop = spark.read.option(\"mode\", \"DROPMALFORMED\").json(\"dbfs:/FileStore/tables/malformed.json\")\n",
    "df_drop.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09769b15-5106-4a5b-9f5b-3aef0a58b36f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    " Deeply Nested JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2e7a5f0-cfcd-43b9-bfe9-ac9a72c321c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- user: struct (nullable = true)\n |    |-- id: long (nullable = true)\n |    |-- profile: struct (nullable = true)\n |    |    |-- contacts: struct (nullable = true)\n |    |    |    |-- email: string (nullable = true)\n |    |    |    |-- phone: string (nullable = true)\n |    |    |-- name: string (nullable = true)\n\n+-----+-----------------+\n|name |email            |\n+-----+-----------------+\n|Alice|alice@example.com|\n+-----+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"multiline\", \"true\").json(\"dbfs:/FileStore/tables/deeplyNested.json\")\n",
    "df.printSchema()\n",
    "\n",
    "df.select(\n",
    "    \"user.profile.name\",\n",
    "    \"user.profile.contacts.email\"\n",
    ").show(truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2025-07-07 15:54:30",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}